{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532e0be8-12be-4418-ba82-d9ccc175c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f3ddef-5992-4093-a3fd-2e7ddf706c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokup\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\datasets\\cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "# DATASETS and DATALOADERS\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = CIFAR10(root=\"C:/Users/gokup/OneDrive/Desktop/PRIME/DL\",download= True ,transform = transforms , train = True) \n",
    "test_dataset = CIFAR10(root=\"C:/Users/gokup/OneDrive/Desktop/PRIME/DL\",download= True , transform= transforms , train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "463e7841-4403-40b2-aa18-6cff8ef6c348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: C:/Users/gokup/OneDrive/Desktop/PRIME/DL\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f62d9cef-dcb3-4449-9e07-f538e3f7babb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: C:/Users/gokup/OneDrive/Desktop/PRIME/DL\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3201f53-99b3-4cbf-aee2-1d8212ac2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader \n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a8415-f2cc-45b7-ae2f-fbc795316180",
   "metadata": {},
   "source": [
    "# Build the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a5308d5-0829-47be-8191-5559809c9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # 1st Layer \n",
    "            nn.Conv2d(3,32,kernel_size=3,padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), \n",
    "\n",
    "            # 2nd Layer \n",
    "            nn.Conv2d(32,64,kernel_size= 3, padding = 1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), \n",
    "\n",
    "            # 3rd Layer \n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2,2))\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(4*4*128,256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,10))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "925833d5-d862-4bab-bba2-a34cbc73d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df04eaf-cf0d-4a87-940b-e8595b79106d",
   "metadata": {},
   "source": [
    "# Train our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b4fb00-06ea-4f03-bc1f-9a4fac4c5958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10 == loss ==  1.389323710937939\n",
      "epoch 2/10 == loss ==  0.9758838884665838\n",
      "epoch 3/10 == loss ==  0.7860757790867935\n",
      "epoch 4/10 == loss ==  0.6517849708609569\n",
      "epoch 5/10 == loss ==  0.5520465678876013\n",
      "epoch 6/10 == loss ==  0.4516798969920334\n",
      "epoch 7/10 == loss ==  0.37069823892067766\n",
      "epoch 8/10 == loss ==  0.2947425155345436\n",
      "epoch 9/10 == loss ==  0.2293431430369082\n",
      "epoch 10/10 == loss ==  0.1905711092474058\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_training_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)  #FP\n",
    "        loss = criterion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_training_loss += loss.item()\n",
    "\n",
    "    print(f\"epoch {epoch+1}/{epochs} == loss ==  {epoch_training_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa64acd-2292-4790-8378-7ec8aaa918dd",
   "metadata": {},
   "source": [
    "# Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9c76139-644b-4c74-97e7-f3db84014337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "7381\n",
      "accuracy -- 73.81\n"
     ]
    }
   ],
   "source": [
    "correct_label = 0 \n",
    "total_label = 0 \n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image , labels in test_loader:\n",
    "        output = model.forward(image)\n",
    "        _,predicted = torch.max(output,1)\n",
    "\n",
    "        correct_label += (predicted == labels).sum().item()\n",
    "        total_label+= labels.size(0)\n",
    "\n",
    "print(total_label)\n",
    "print(correct_label)\n",
    "print(\"accuracy --\",(correct_label/total_label)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3c97e-1efc-4727-9c60-16be735e1434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
