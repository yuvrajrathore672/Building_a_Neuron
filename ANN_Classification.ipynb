{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b8c4e0-5a34-4723-9747-28f92d56a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader , TensorDataset\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ccc13fd-3d5c-413c-8b86-b389f8df3301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>MAJOR_AXIS</th>\n",
       "      <th>MINOR_AXIS</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>EQDIASQ</th>\n",
       "      <th>SOLIDITY</th>\n",
       "      <th>CONVEX_AREA</th>\n",
       "      <th>EXTENT</th>\n",
       "      <th>ASPECT_RATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>KurtosisRR</th>\n",
       "      <th>KurtosisRG</th>\n",
       "      <th>KurtosisRB</th>\n",
       "      <th>EntropyRR</th>\n",
       "      <th>EntropyRG</th>\n",
       "      <th>EntropyRB</th>\n",
       "      <th>ALLdaub4RR</th>\n",
       "      <th>ALLdaub4RG</th>\n",
       "      <th>ALLdaub4RB</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422163</td>\n",
       "      <td>2378.908</td>\n",
       "      <td>837.8484</td>\n",
       "      <td>645.6693</td>\n",
       "      <td>0.6373</td>\n",
       "      <td>733.1539</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>424428</td>\n",
       "      <td>0.7831</td>\n",
       "      <td>1.2976</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2370</td>\n",
       "      <td>2.9574</td>\n",
       "      <td>4.2287</td>\n",
       "      <td>-59191263232</td>\n",
       "      <td>-50714214400</td>\n",
       "      <td>-39922372608</td>\n",
       "      <td>58.7255</td>\n",
       "      <td>54.9554</td>\n",
       "      <td>47.8400</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>338136</td>\n",
       "      <td>2085.144</td>\n",
       "      <td>723.8198</td>\n",
       "      <td>595.2073</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>656.1464</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>339014</td>\n",
       "      <td>0.7795</td>\n",
       "      <td>1.2161</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6228</td>\n",
       "      <td>2.6350</td>\n",
       "      <td>3.1704</td>\n",
       "      <td>-34233065472</td>\n",
       "      <td>-37462601728</td>\n",
       "      <td>-31477794816</td>\n",
       "      <td>50.0259</td>\n",
       "      <td>52.8168</td>\n",
       "      <td>47.8315</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>526843</td>\n",
       "      <td>2647.394</td>\n",
       "      <td>940.7379</td>\n",
       "      <td>715.3638</td>\n",
       "      <td>0.6494</td>\n",
       "      <td>819.0222</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>528876</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>1.3150</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7516</td>\n",
       "      <td>3.8611</td>\n",
       "      <td>4.7192</td>\n",
       "      <td>-93948354560</td>\n",
       "      <td>-74738221056</td>\n",
       "      <td>-60311207936</td>\n",
       "      <td>65.4772</td>\n",
       "      <td>59.2860</td>\n",
       "      <td>51.9378</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>416063</td>\n",
       "      <td>2351.210</td>\n",
       "      <td>827.9804</td>\n",
       "      <td>645.2988</td>\n",
       "      <td>0.6266</td>\n",
       "      <td>727.8378</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>418255</td>\n",
       "      <td>0.7759</td>\n",
       "      <td>1.2831</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0401</td>\n",
       "      <td>8.6136</td>\n",
       "      <td>8.2618</td>\n",
       "      <td>-32074307584</td>\n",
       "      <td>-32060925952</td>\n",
       "      <td>-29575010304</td>\n",
       "      <td>43.3900</td>\n",
       "      <td>44.1259</td>\n",
       "      <td>41.1882</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>347562</td>\n",
       "      <td>2160.354</td>\n",
       "      <td>763.9877</td>\n",
       "      <td>582.8359</td>\n",
       "      <td>0.6465</td>\n",
       "      <td>665.2291</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>350797</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>1.3108</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7016</td>\n",
       "      <td>2.9761</td>\n",
       "      <td>4.4146</td>\n",
       "      <td>-39980974080</td>\n",
       "      <td>-35980042240</td>\n",
       "      <td>-25593278464</td>\n",
       "      <td>52.7743</td>\n",
       "      <td>50.9080</td>\n",
       "      <td>42.6666</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n",
       "0  422163   2378.908    837.8484    645.6693        0.6373  733.1539   \n",
       "1  338136   2085.144    723.8198    595.2073        0.5690  656.1464   \n",
       "2  526843   2647.394    940.7379    715.3638        0.6494  819.0222   \n",
       "3  416063   2351.210    827.9804    645.2988        0.6266  727.8378   \n",
       "4  347562   2160.354    763.9877    582.8359        0.6465  665.2291   \n",
       "\n",
       "   SOLIDITY  CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  KurtosisRR  KurtosisRG  \\\n",
       "0    0.9947       424428  0.7831        1.2976  ...      3.2370      2.9574   \n",
       "1    0.9974       339014  0.7795        1.2161  ...      2.6228      2.6350   \n",
       "2    0.9962       528876  0.7657        1.3150  ...      3.7516      3.8611   \n",
       "3    0.9948       418255  0.7759        1.2831  ...      5.0401      8.6136   \n",
       "4    0.9908       350797  0.7569        1.3108  ...      2.7016      2.9761   \n",
       "\n",
       "   KurtosisRB    EntropyRR    EntropyRG    EntropyRB  ALLdaub4RR  ALLdaub4RG  \\\n",
       "0      4.2287 -59191263232 -50714214400 -39922372608     58.7255     54.9554   \n",
       "1      3.1704 -34233065472 -37462601728 -31477794816     50.0259     52.8168   \n",
       "2      4.7192 -93948354560 -74738221056 -60311207936     65.4772     59.2860   \n",
       "3      8.2618 -32074307584 -32060925952 -29575010304     43.3900     44.1259   \n",
       "4      4.4146 -39980974080 -35980042240 -25593278464     52.7743     50.9080   \n",
       "\n",
       "   ALLdaub4RB  Class  \n",
       "0     47.8400  BERHI  \n",
       "1     47.8315  BERHI  \n",
       "2     51.9378  BERHI  \n",
       "3     41.1882  BERHI  \n",
       "4     42.6666  BERHI  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing dataset \n",
    "\n",
    "df = pd.read_csv(\"C:/Users/gokup/Downloads/DateFruit_Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abdf827e-84e3-4da6-b319-19a543c0cab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(898, 35)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd28a43-cafb-40da-9c62-3e677e49d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Splitting our df \n",
    "\n",
    "X = df.drop(\"Class\",axis=1)\n",
    "y = df[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400e94fc-7409-4461-a70d-45c064e4790e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BERHI', 'DEGLET', 'DOKOL', 'IRAQI', 'ROTANA', 'SAFAVI', 'SOGAY'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y.value_counts()\n",
    "\n",
    "df[\"Class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d3ed7df-af47-460f-ade4-7d0cc77f2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before spliting  -- Encoding \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af4cb115-310b-47f6-87ec-06ee72653a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(\n",
    "    X , y , random_state= 42, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd906834-fe2c-4d76-a21a-078be9003182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Scaling our df\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22f63e10-158b-48b4-922a-b38af39fd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting df into tensor \n",
    "\n",
    "x_train_tensor = torch.tensor(x_train_scaled,dtype = torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train ,dtype=torch.long)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_scaled,dtype= torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a70514-1f0d-46de-bd1a-79bd9ccecc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Tensor Dataset \n",
    "\n",
    "train_tensor = TensorDataset(x_train_tensor,y_train_tensor)\n",
    "test_tensor = TensorDataset(x_test_tensor,y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8212bc80-d8eb-4e50-87de-b7c7c41c3b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DataLoader ---- batch creating \n",
    "\n",
    "train_loader = DataLoader(train_tensor,batch_size= 32, shuffle = True)\n",
    "test_loader = DataLoader(test_tensor,batch_size= 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733174a-6a3b-4443-8dfd-3efa70bb4829",
   "metadata": {},
   "source": [
    "## ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8152a8a2-40ff-407b-9c0d-af7ee83815bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our model --- \n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN,self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # 1st Hidden Layer \n",
    "            nn.Linear(X.shape[1],64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2 hidden Layer\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Output layer\n",
    "            nn.Linear(64,7)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11ffd29b-490e-4062-b1ad-a9b4a8df2122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model --- \n",
    "model = ANN()\n",
    "\n",
    "# Loss fnx and Optimizer \n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd5bf2ae-a2d4-4356-a84e-46041043a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100 >== train loss = 1.68777030447255\n",
      "epoch 2/100 >== train loss = 1.0864973586538564\n",
      "epoch 3/100 >== train loss = 0.7374417315358701\n",
      "epoch 4/100 >== train loss = 0.5554129779338837\n",
      "epoch 5/100 >== train loss = 0.4645652589590653\n",
      "epoch 6/100 >== train loss = 0.4022444583799528\n",
      "epoch 7/100 >== train loss = 0.3589683032554129\n",
      "epoch 8/100 >== train loss = 0.32869929852692975\n",
      "epoch 9/100 >== train loss = 0.31211804501388385\n",
      "epoch 10/100 >== train loss = 0.28534385367580084\n",
      "epoch 11/100 >== train loss = 0.2619681215804556\n",
      "epoch 12/100 >== train loss = 0.24783031610043152\n",
      "epoch 13/100 >== train loss = 0.23226185142993927\n",
      "epoch 14/100 >== train loss = 0.22645543353713077\n",
      "epoch 15/100 >== train loss = 0.21247939603484195\n",
      "epoch 16/100 >== train loss = 0.20375274866819382\n",
      "epoch 17/100 >== train loss = 0.2027296851510587\n",
      "epoch 18/100 >== train loss = 0.1912381551835848\n",
      "epoch 19/100 >== train loss = 0.17922430420699326\n",
      "epoch 20/100 >== train loss = 0.17466710378294406\n",
      "epoch 21/100 >== train loss = 0.16988714071719543\n",
      "epoch 22/100 >== train loss = 0.1763071803295094\n",
      "epoch 23/100 >== train loss = 0.1600451906738074\n",
      "epoch 24/100 >== train loss = 0.15222564550197643\n",
      "epoch 25/100 >== train loss = 0.14353215192323146\n",
      "epoch 26/100 >== train loss = 0.1415263304243917\n",
      "epoch 27/100 >== train loss = 0.13551596114816872\n",
      "epoch 28/100 >== train loss = 0.13770143266605295\n",
      "epoch 29/100 >== train loss = 0.1330230117815992\n",
      "epoch 30/100 >== train loss = 0.12864737795746845\n",
      "epoch 31/100 >== train loss = 0.13065171257957167\n",
      "epoch 32/100 >== train loss = 0.1225430092409901\n",
      "epoch 33/100 >== train loss = 0.1158269862441913\n",
      "epoch 34/100 >== train loss = 0.11582326297850712\n",
      "epoch 35/100 >== train loss = 0.11243719267456428\n",
      "epoch 36/100 >== train loss = 0.10989767094345196\n",
      "epoch 37/100 >== train loss = 0.1090814267973537\n",
      "epoch 38/100 >== train loss = 0.10182524359096652\n",
      "epoch 39/100 >== train loss = 0.10142151865622272\n",
      "epoch 40/100 >== train loss = 0.11025517709229303\n",
      "epoch 41/100 >== train loss = 0.09829675155165403\n",
      "epoch 42/100 >== train loss = 0.094322486659107\n",
      "epoch 43/100 >== train loss = 0.09468133779971497\n",
      "epoch 44/100 >== train loss = 0.08939172108860119\n",
      "epoch 45/100 >== train loss = 0.10044661585403525\n",
      "epoch 46/100 >== train loss = 0.08716935036784929\n",
      "epoch 47/100 >== train loss = 0.08716072919576065\n",
      "epoch 48/100 >== train loss = 0.08117362419548242\n",
      "epoch 49/100 >== train loss = 0.07722050344328517\n",
      "epoch 50/100 >== train loss = 0.0808774252300677\n",
      "epoch 51/100 >== train loss = 0.07992374483981858\n",
      "epoch 52/100 >== train loss = 0.0775177599457295\n",
      "epoch 53/100 >== train loss = 0.07664368447402249\n",
      "epoch 54/100 >== train loss = 0.0751131738981475\n",
      "epoch 55/100 >== train loss = 0.07284948622564906\n",
      "epoch 56/100 >== train loss = 0.06848253722748031\n",
      "epoch 57/100 >== train loss = 0.06572778122094662\n",
      "epoch 58/100 >== train loss = 0.06558031167673029\n",
      "epoch 59/100 >== train loss = 0.0685517686702635\n",
      "epoch 60/100 >== train loss = 0.06323159251200117\n",
      "epoch 61/100 >== train loss = 0.06206224745382433\n",
      "epoch 62/100 >== train loss = 0.059273646658529404\n",
      "epoch 63/100 >== train loss = 0.05959144339937231\n",
      "epoch 64/100 >== train loss = 0.055310426603840744\n",
      "epoch 65/100 >== train loss = 0.05543107604203017\n",
      "epoch 66/100 >== train loss = 0.05473851372042428\n",
      "epoch 67/100 >== train loss = 0.053197555729876396\n",
      "epoch 68/100 >== train loss = 0.05064643372822067\n",
      "epoch 69/100 >== train loss = 0.05279105533238338\n",
      "epoch 70/100 >== train loss = 0.04893863363110501\n",
      "epoch 71/100 >== train loss = 0.052528268495655575\n",
      "epoch 72/100 >== train loss = 0.053771095026446426\n",
      "epoch 73/100 >== train loss = 0.04594517683448351\n",
      "epoch 74/100 >== train loss = 0.0475518601420133\n",
      "epoch 75/100 >== train loss = 0.04428598110604545\n",
      "epoch 76/100 >== train loss = 0.048134807699724384\n",
      "epoch 77/100 >== train loss = 0.04249159797378208\n",
      "epoch 78/100 >== train loss = 0.044297181150835495\n",
      "epoch 79/100 >== train loss = 0.04012734325521666\n",
      "epoch 80/100 >== train loss = 0.04122825388027274\n",
      "epoch 81/100 >== train loss = 0.04246223264414331\n",
      "epoch 82/100 >== train loss = 0.0383393259557045\n",
      "epoch 83/100 >== train loss = 0.042057602868779846\n",
      "epoch 84/100 >== train loss = 0.0405332422207879\n",
      "epoch 85/100 >== train loss = 0.035416515088518674\n",
      "epoch 86/100 >== train loss = 0.03310879482887685\n",
      "epoch 87/100 >== train loss = 0.03159613357654408\n",
      "epoch 88/100 >== train loss = 0.03427618025275676\n",
      "epoch 89/100 >== train loss = 0.03397292165976504\n",
      "epoch 90/100 >== train loss = 0.03134904390848849\n",
      "epoch 91/100 >== train loss = 0.030521103484637064\n",
      "epoch 92/100 >== train loss = 0.0348286176626773\n",
      "epoch 93/100 >== train loss = 0.037112353248116764\n",
      "epoch 94/100 >== train loss = 0.033077631651869284\n",
      "epoch 95/100 >== train loss = 0.029224091837101656\n",
      "epoch 96/100 >== train loss = 0.03127072044931676\n",
      "epoch 97/100 >== train loss = 0.028544818435836096\n",
      "epoch 98/100 >== train loss = 0.025413322019512238\n",
      "epoch 99/100 >== train loss = 0.02573929371757676\n",
      "epoch 100/100 >== train loss = 0.02465121381227737\n"
     ]
    }
   ],
   "source": [
    "# Training our ANN \n",
    "\n",
    "train_loss = []\n",
    "# best_loss = []\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for xb , yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        output = model(xb)\n",
    "        loss = criteria(output,yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()      # parameter update\n",
    "\n",
    "        running_loss+=loss.item()\n",
    "\n",
    "    epoch_train_loss = running_loss/len(train_loader)\n",
    "    train_loss.append(epoch_train_loss)\n",
    "\n",
    "    print(f\"epoch {epoch+1}/{epochs} >== train loss = {epoch_train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e8f1c12-be6b-42bb-a33a-cf07ad47001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total value  180\n",
      "Correct 170\n",
      "accuracy 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Evaluation -- \n",
    "\n",
    "model.eval()\n",
    "total = 0 \n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        output = model(xb) # Which gives output  -- sum != 1\n",
    "        max_val , predicted = torch.max(output,1)\n",
    "    \n",
    "        correct += (predicted == yb).sum().item()     #sum of values which are correctly predicted (same as yb)\n",
    "        total += yb.size(0)     #return actual samplen in each batch\n",
    "\n",
    " \n",
    "print(\"Total value \", total)\n",
    "print(\"Correct\", correct)\n",
    "print(\"accuracy\",correct/total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358f008-8bb5-45c7-a5da-aae4b4f8e1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18a6ea7b-c9cd-43b8-b754-b799cbfb899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "x_train_pca = pca.fit_transform(x_train_scaled)\n",
    "x_test_pca = pca.transform(x_test_scaled)\n",
    "\n",
    "\n",
    "# converting pca to tensor -- \n",
    "x_train_pca_tensor = torch.tensor(x_train_pca,dtype = torch.float32) \n",
    "x_test_pca_tensor = torch.tensor(x_test_pca,dtype=torch.float32) \n",
    "\n",
    "y_train_pca_tensor = torch.tensor(y_train,dtype= torch.long)\n",
    "y_test_pca_tensor = torch.tensor(y_test,dtype = torch.long)\n",
    "\n",
    "\n",
    "# tensor dataset \n",
    "pca_train_tensor = TensorDataset(x_train_pca_tensor, y_train_pca_tensor)\n",
    "pca_test_tensor = TensorDataset(x_test_pca_tensor,y_test_pca_tensor)\n",
    "\n",
    "# Dataset Loader \n",
    "pca_train_loader = DataLoader(pca_train_tensor,batch_size=32,shuffle=True)\n",
    "pca_test_loader = DataLoader(pca_test_tensor,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0cf018b-118d-4275-9b67-a65da3fdd0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(718, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d187937-4b1f-409c-8b59-e2035ad66dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define ANN_ \n",
    "class PANN(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(PANN,self).__init__()\n",
    "\n",
    "        self.pca_model = nn.Sequential(\n",
    "            nn.Linear(9,64), \n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64,7),   \n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.pca_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a572d4c-d837-47c0-9e8d-b8398aac0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = PANN()\n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(p_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e12f54b-bf99-40c1-a25d-9337f3928db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1/100 >== p_train_loss : 1.5749850480452827\n",
      "epochs 2/100 >== p_train_loss : 1.0028354007264841\n",
      "epochs 3/100 >== p_train_loss : 0.7060836644276328\n",
      "epochs 4/100 >== p_train_loss : 0.5376273043777632\n",
      "epochs 5/100 >== p_train_loss : 0.44394659736882086\n",
      "epochs 6/100 >== p_train_loss : 0.38328519463539124\n",
      "epochs 7/100 >== p_train_loss : 0.3414960788643878\n",
      "epochs 8/100 >== p_train_loss : 0.3154804667700892\n",
      "epochs 9/100 >== p_train_loss : 0.2875427674988042\n",
      "epochs 10/100 >== p_train_loss : 0.2712429269500401\n",
      "epochs 11/100 >== p_train_loss : 0.2572819266630256\n",
      "epochs 12/100 >== p_train_loss : 0.24778413157100262\n",
      "epochs 13/100 >== p_train_loss : 0.23987027225287064\n",
      "epochs 14/100 >== p_train_loss : 0.2355820646752482\n",
      "epochs 15/100 >== p_train_loss : 0.22246847917204318\n",
      "epochs 16/100 >== p_train_loss : 0.21353784705633702\n",
      "epochs 17/100 >== p_train_loss : 0.20889265770497528\n",
      "epochs 18/100 >== p_train_loss : 0.2031456617557484\n",
      "epochs 19/100 >== p_train_loss : 0.19340098616869553\n",
      "epochs 20/100 >== p_train_loss : 0.1895305349127106\n",
      "epochs 21/100 >== p_train_loss : 0.1876061996040137\n",
      "epochs 22/100 >== p_train_loss : 0.1814685514115769\n",
      "epochs 23/100 >== p_train_loss : 0.1914167835012726\n",
      "epochs 24/100 >== p_train_loss : 0.1732847113972125\n",
      "epochs 25/100 >== p_train_loss : 0.17747412233249002\n",
      "epochs 26/100 >== p_train_loss : 0.1656338825174\n",
      "epochs 27/100 >== p_train_loss : 0.16142390295863152\n",
      "epochs 28/100 >== p_train_loss : 0.1627594802690589\n",
      "epochs 29/100 >== p_train_loss : 0.15332540254230084\n",
      "epochs 30/100 >== p_train_loss : 0.15143941245649173\n",
      "epochs 31/100 >== p_train_loss : 0.1544286926155505\n",
      "epochs 32/100 >== p_train_loss : 0.15339002256160197\n",
      "epochs 33/100 >== p_train_loss : 0.14794088948680006\n",
      "epochs 34/100 >== p_train_loss : 0.1525407054502031\n",
      "epochs 35/100 >== p_train_loss : 0.13885075372198355\n",
      "epochs 36/100 >== p_train_loss : 0.13813409176857575\n",
      "epochs 37/100 >== p_train_loss : 0.13318366873199525\n",
      "epochs 38/100 >== p_train_loss : 0.13834911422884982\n",
      "epochs 39/100 >== p_train_loss : 0.12945366577933665\n",
      "epochs 40/100 >== p_train_loss : 0.13095453170978505\n",
      "epochs 41/100 >== p_train_loss : 0.12843641800724942\n",
      "epochs 42/100 >== p_train_loss : 0.12272319375820782\n",
      "epochs 43/100 >== p_train_loss : 0.12341871031600496\n",
      "epochs 44/100 >== p_train_loss : 0.12320449883523195\n",
      "epochs 45/100 >== p_train_loss : 0.12292603613889735\n",
      "epochs 46/100 >== p_train_loss : 0.12226748612263928\n",
      "epochs 47/100 >== p_train_loss : 0.12275683248172635\n",
      "epochs 48/100 >== p_train_loss : 0.11830745702204497\n",
      "epochs 49/100 >== p_train_loss : 0.11349917883458345\n",
      "epochs 50/100 >== p_train_loss : 0.1098318769717994\n",
      "epochs 51/100 >== p_train_loss : 0.10430647429261032\n",
      "epochs 52/100 >== p_train_loss : 0.10916942746742912\n",
      "epochs 53/100 >== p_train_loss : 0.11199224140980969\n",
      "epochs 54/100 >== p_train_loss : 0.10495159564458806\n",
      "epochs 55/100 >== p_train_loss : 0.10357436618727187\n",
      "epochs 56/100 >== p_train_loss : 0.10094271786510944\n",
      "epochs 57/100 >== p_train_loss : 0.098313694819808\n",
      "epochs 58/100 >== p_train_loss : 0.094462299881422\n",
      "epochs 59/100 >== p_train_loss : 0.09339325664483983\n",
      "epochs 60/100 >== p_train_loss : 0.0999929751067058\n",
      "epochs 61/100 >== p_train_loss : 0.09409990655663221\n",
      "epochs 62/100 >== p_train_loss : 0.09432975591524788\n",
      "epochs 63/100 >== p_train_loss : 0.09089416824281216\n",
      "epochs 64/100 >== p_train_loss : 0.09001218227912551\n",
      "epochs 65/100 >== p_train_loss : 0.08645835178701775\n",
      "epochs 66/100 >== p_train_loss : 0.09736883899439937\n",
      "epochs 67/100 >== p_train_loss : 0.09953180530472942\n",
      "epochs 68/100 >== p_train_loss : 0.09350226625152257\n",
      "epochs 69/100 >== p_train_loss : 0.08400752703132837\n",
      "epochs 70/100 >== p_train_loss : 0.08259888430652411\n",
      "epochs 71/100 >== p_train_loss : 0.08309648299346799\n",
      "epochs 72/100 >== p_train_loss : 0.08069800269668517\n",
      "epochs 73/100 >== p_train_loss : 0.07705234724056462\n",
      "epochs 74/100 >== p_train_loss : 0.08367722756836725\n",
      "epochs 75/100 >== p_train_loss : 0.07641914486885071\n",
      "epochs 76/100 >== p_train_loss : 0.07520275328146375\n",
      "epochs 77/100 >== p_train_loss : 0.07488085580584795\n",
      "epochs 78/100 >== p_train_loss : 0.07718554701980042\n",
      "epochs 79/100 >== p_train_loss : 0.07416493437536385\n",
      "epochs 80/100 >== p_train_loss : 0.06797175530506216\n",
      "epochs 81/100 >== p_train_loss : 0.06761498668271562\n",
      "epochs 82/100 >== p_train_loss : 0.06934653105133254\n",
      "epochs 83/100 >== p_train_loss : 0.06587212251337327\n",
      "epochs 84/100 >== p_train_loss : 0.06333018730030111\n",
      "epochs 85/100 >== p_train_loss : 0.06486529221191355\n",
      "epochs 86/100 >== p_train_loss : 0.06532449259058289\n",
      "epochs 87/100 >== p_train_loss : 0.058828946850869965\n",
      "epochs 88/100 >== p_train_loss : 0.0587228059100554\n",
      "epochs 89/100 >== p_train_loss : 0.058232765320850456\n",
      "epochs 90/100 >== p_train_loss : 0.05859479097568471\n",
      "epochs 91/100 >== p_train_loss : 0.0601441309503887\n",
      "epochs 92/100 >== p_train_loss : 0.05508895987725776\n",
      "epochs 93/100 >== p_train_loss : 0.05707611568758021\n",
      "epochs 94/100 >== p_train_loss : 0.052888605784138905\n",
      "epochs 95/100 >== p_train_loss : 0.05205156864679378\n",
      "epochs 96/100 >== p_train_loss : 0.0548067779239753\n",
      "epochs 97/100 >== p_train_loss : 0.059131616443071675\n",
      "epochs 98/100 >== p_train_loss : 0.050746702366387064\n",
      "epochs 99/100 >== p_train_loss : 0.0521240205625477\n",
      "epochs 100/100 >== p_train_loss : 0.050546703131302544\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "p_train_loss = []\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    p_model.train()\n",
    "    p_running_loss = 0 \n",
    "\n",
    "    for xb,yb in pca_train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = p_model(xb)\n",
    "        loss= criteria(outputs,yb) \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        p_running_loss += loss.item() \n",
    "    \n",
    "    pca_epoch_loss = p_running_loss/len(pca_train_loader)\n",
    "    p_train_loss.append(pca_epoch_loss)\n",
    "\n",
    "    print(f\"epochs {epoch+1}/{epochs} >== p_train_loss : {pca_epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "523b3577-512b-4a04-a7be-123cf32eb849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9055555555555556\n",
      "180 total \n",
      "163 correct\n"
     ]
    }
   ],
   "source": [
    "# Evalution \n",
    "\n",
    "p_model.eval()\n",
    "total= 0 \n",
    "correct = 0 \n",
    "\n",
    "with torch.no_grad(): \n",
    "    for xb,yb in pca_test_loader: \n",
    "        outputs = p_model(xb) \n",
    "        max_val , predicted = torch.max(outputs,1)\n",
    "        \n",
    "\n",
    "        total += yb.size(0)\n",
    "        correct += (predicted==yb).sum().item()\n",
    "\n",
    "print(\"accuracy\" , (correct/total))\n",
    "print(total , \"total \")\n",
    "print(correct, \"correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2000f94-f9ca-43c2-9c2f-c0bf39e4d49d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
